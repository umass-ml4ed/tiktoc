##################################################
# exp_opts
##################################################
seed: 0
exp_name: 'okt' 
save_model: false
testing: false # only use a very small portion of the dataset for testing purposes
log_train_every_itr: 10
model_save_dir: 'checkpoints'
use_cuda: true
log_wandb: false
continue_training: false
start_epoch: 0
wandb_project: 'okt-multitask-cv'
wandb_key: 'ff70920d9852a9d2e78bbd1cd2e100154d2c9c7d' #Change to your own api key
##################################################
# data_opts
##################################################
data_path: 'data'
test_size: 0.2 # percentage of test dataset
max_len: 200 # maximum number of submission per student 
label_type: 'raw' # score division category, choose from 'binary', 'tenary' or 'raw'
first_ast_convertible: true # whether to use student first submission to each question
split_by: 'student'
use_template: false
##################################################
# model_lstm_opts
##################################################
use_lstm: true
lstm_inp_dim: 968 # 968: 768+200 (200 is the astnn embedding) if generator is gpt-2, otherwise set to 4096 when creating the model
lstm_hid_dim: 64
train_lstm: true
num_layers: 1
lstm_lr: 0.00005
##################################################
# model_gpt_opts
##################################################
okt_model: 'meta-llama/Meta-Llama-3-8B-Instruct' # pre-trained GPT model: choose from 'student', 'funcom', 'gpt-2', 'codellama/CodeLlama-7b-Instruct-hf', 'meta-llama/Meta-Llama-3-8B-Instruct', or 'Qwen/Qwen1.5-7B'
train_okt: true
##################################################
# train_generator_opts
##################################################
epochs: 10
batch_size: 2
accum_iter: 1
lr: 0.00001
lr_linear: 0.005
use_scheduler: true
warmup_ratio: 0.1
multitask: true
multitask_pred_linear: 0.0001
loss_fn: 'BCE'
multitask_label: 'granular'
multitask_inp: 'hid'
multitask_init: 'emb'
multitask_init_combine: false
use_transition_model: false
alpha: ''
valid_question_no: 17
T_max: 26
##################################################
# LoRA configs
##################################################
lora_alpha: 256
lora_dropout: 0.05
lora_r: 128
##################################################
# evaluation configs
##################################################
nsamples: 1
k: 1
p: 0
max_new_tokens: 400
change_generation_length: false